---
title: "Getting Textual Data: Web Scraping"
subtitle: "Dr. Yotam Ophir's Class"
author: "Ayse D. Lokmanoglu"
date: "`r format(Sys.time(), '%d %B, %Y')`"
output:
  xaringan::moon_reader:
    css: xaringan-themer.css
    self_contained: true
    nature:
      slideNumberFormat: "%current%"
      highlightStyle: github
      highlightLines: true
      ratio: 3:2
      countIncrementalSlides: false
urlcolor: blue
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
options(htmltools.dir.version = FALSE)
knitr::opts_chunk$set(
  fig.width=9, fig.height=3.5, fig.retina=3,
  out.width = "100%",
  cache = TRUE,
  dev = "svg",      
  echo = TRUE,
  message = FALSE, 
  warning = FALSE,
  fig.show = TRUE,
  hiline = TRUE      
)

knitr::opts_chunk$set(echo = TRUE, message = FALSE, warning = FALSE,
                      comment = "#>", highlight = TRUE,
                      fig.align = "center")
```

```{r xaringan-themer, include=FALSE, warning=FALSE}
library(xaringanthemer)
style_duo_accent(
  primary_color = "#1c5253",
  secondary_color = "#646464",
  inverse_header_color = "#FFFFFF",
  base_font_size = "20px",                      # increase text font size
  code_font_size = ".8rem",                      # make code font size normal
  header_font_google = google_font("Josefin Sans"),
  text_font_google   = google_font("Montserrat", "300", "300i"),
  code_font_google   = google_font("Fira Mono"),
  extra_css = list(
    ".small" = list("font-size" = ".78rem"),    # for smaller text when needed
    ".big"  = list("font-size" = "1.2rem"),       # for bigger text when needed  
    ".small-code pre code" = list("font-size" = ".72rem"),  # small code
    ".tiny-code pre code" = list("font-size" = ".6rem"),    # tiny code
    "li" = list("padding" = "8px 0px 0px"),               # more list padding
    "table th, table td" = list("padding" = "8px")      # table cell padding
  )
)
```

```{r xaringan-extra, echo=FALSE, include=FALSE}
xaringanExtra::use_clipboard(button_text = "Copy")
```

## Web Scraping using R
* Extracting of data from a website
   + Including platforms that have web interfaces
![The Net](https://github.com/aysedeniz09/Intro_Comp_Social_Science/blob/main/images/the-net-social.jpeg?raw=true)
The Net (1995) Sandra Bullock
---

## Ethical & legal scraping?
* Depends on where you are in the world. 
   + [EU law on scraping](https://www.lexisnexis.co.uk/legal/guidance/what-is-the-eu-law-on-data-scraping-from-websites)
   + Terms of services/use
      * i.e. [Instagram ToU](https://help.instagram.com/581066165581870): “You can't attempt to create accounts or access or collect information in unauthorized ways. This includes creating accounts or collecting information in an automated way without our express permission”


**‘Publicly available data’ or ‘Open-Source’**

---

![takenmeme](https://github.com/aysedeniz09/Intro_Comp_Social_Science/blob/main/images/taken-meme.jpeg?raw=true)

---

## Hacking vs Scraping – where is the line?
* Meta v Social Data Trading Ltd. (12/8/2022)
   + Meta is suing Social Data Trading a Hong Kong based company on violating their terms of services in Facebook and Instagram. Meta alleges that after they block Social Data Trading from both platforms the company by creating fake accounts continued to collect data. They allege this is hacking under Section 502 of California’s Penal Code.
* hiQ Labs, Inc. v. LinkedIn Corp (4/18/2022)
   + “Computer Fraud and Abuse Act (CFAA), a federal anti-hacking law that prohibits unauthorized access to computer systems, does not apply to websites that are open to the public.”[1] 
   
   

[1]: Maurer, R. (2022, May 2). Scraping Public Data from LinkedIn Is Legal. SHRM. https://www.shrm.org/resourcesandtools/hr-topics/technology/pages/scraping-public-data-from-linkedin-is-legal.aspx 


---

### Before we code

#### Hypertext Markup Language

[HTML](https://en.wikipedia.org/wiki/HTML) is a markup language to be viewed on web browsers. HTML is often assisted by Cascading Style Sheets (CSS) or JavaScript. 

Building blocks:
* Elements
* Tags
* Attributes

Reference: [Wikipedia HTML Element](https://en.wikipedia.org/wiki/HTML_element)

---

HTML Element Example

![htmlelement](https://github.com/aysedeniz09/Intro_Comp_Social_Science/blob/main/images/htmlelement.jpg?raw=true)

---

HTML Snippet Example

![htmlsnippet](https://github.com/aysedeniz09/Intro_Comp_Social_Science/blob/main/images/html%20snippet.png?raw=true)

---

### CSS Selector

* CSS is a style sheet that will help us find elements in the webpage


Simple CSS selectors:
* ID
* Class
* Name

--

#### How can we find CSS on a webpage? 
1. In Google Chrome right click and select "Inspect"
   a. View html elements
   b. Select an element
   c. View CSS selectors
   d. Copy selector
3. [Selector Gadget](https://selectorgadget.com/)

[Reference for CSS Selectors](https://www.w3schools.com/cssref/css_selectors.php)

---
### Example for Google Chrome
.center[![chromecss](https://github.com/aysedeniz09/Intro_Comp_Social_Science/blob/main/images/JamesLecturetest.jpg?raw=true)]

---

[R Web Scraping Cheat Sheet](https://github.com/yusuzech/r-web-scraping-cheat-sheet/blob/master/README.md)
.center[![rvest](https://github.com/yusuzech/r-web-scraping-cheat-sheet/blob/master/resources/functions_and_classes.png?raw=true)]

---

## Let's start coding!
First we install all the packages! You only have to do this once, for all other times you just need to load the packages (see next slide)
.small-code[
```{r needed-packages, eval=FALSE}
# this code is to install all the packages, you only need to run this once, afterwards all you need is to load the packages 
install.packages(c(
  "tidyverse",   # just tidy
  "dplyr",       # wrangle data
  "purrr",       # iterate and map
  "scales",      # scale functions for visualizations
  "ggthemes",    # graph theme options
  "jtools",      # ggplot2 themes
  "ggplot2",     # visualization
  "lubridate",   # for dates
  "zoo",         # another package for dates
  "rvest",       # the scraping package
  "httr",        # website       
  "flextable",   # for tables
  "huxtable",    # also for tables
  "stringr"      # manipulate text
  ))
```
]

---

### Load packages
.small-code[
```{r load-packages, warning=FALSE}
library(tidyverse)
library(dplyr)
library(purrr)
library(scales)
library(ggthemes)
library(lubridate)
library(jtools)
library(zoo)
library(rvest)
library(ggplot2)
library(httr)
library(flextable)
library(huxtable)
library(stringr)
```
]

Today we will mostly use the [Rvest package](https://rvest.tidyverse.org/)

How to find details on packages?
Type the package name preceded by ? and you will see the package details on the help window

.small-code[
```{r package-help, eval=FALSE, warning=FALSE}
?rvest
```
]

---

Rvest Core functions we will be using today:

| Function            | Description                                                       |
|---------------------|-------------------------------------------------------------------|
| `read_html()`       | read HTML from a character string of url, returns xml_document    |
| `html_element()`    | find HTML element using CSS selectors                             |
| `html_elements()`   | find HTML elements using CSS selectors                            |
| `html_attr()`       | gets single attribute                                             |
| `html_attrs()`      | gets all attributes                                               |
| `html_table()`      | parse an HTML table into a data frame                             |
| `html_text()`       | thin wrapper which returns raw text                               |
| `html_text2()`      | slower than html_text but returns cleaner text                    |


---

Let's do one manual example before going online

URL: [https://aysedeniz09.github.io/](https://aysedeniz09.github.io/)

.tiny[
```html
<html lang="en"><head>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home">
  
<p>Additional Information: Ayşe is pronounced eye-shea</p> #<<

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
```
]

```{r html-ayse, echo=FALSE}
html_ayse <- '<html lang="en"><head>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home">
  
<p>Additional Information: Ayşe is pronounced eye-shea</p> #<<

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>'

```

--

I want to scrape just how my name is pronounced (highlighted in yellow):

.tiny[
```{r eval=FALSE}
<html lang="en"><head>
<main class="page-content" aria-label="Content">
      <div class="wrapper">
        <div class="home">
  
<p>Additional Information: Ayşe is pronounced eye-shea</p> #<<

</body><grammarly-desktop-integration data-grammarly-shadow-root="true"></grammarly-desktop-integration></html>
```
]

---

**Step 1**

Save the HTML as a character object named `html_ayse`.

```{r eval=FALSE}
html_ayse <- "<html lang="en"><head>...</html>"
```

--

**Step 2**

To extract all `<p>` elements:

```{r}
html_ayse %>% 
  read_html() %>% 
  html_nodes(css = "p") #<<
```

--

**Step 3**

To extract the contents between the tags:

```{r}
html_ayse %>% 
  read_html() %>% 
  html_nodes(css = "p") %>% 
  html_text2() #<<
```

---

### The Website

Check if you can scrape the website by appending /robots.txt to the url:

.center[![wiki](https://github.com/aysedeniz09/Intro_Comp_Social_Science/blob/main/images/Screenshot%202023-03-09%20at%203.31.41%20PM.png?raw=true)]

Can we scrape Wikipedia? 

---

### Let's Scrape!

Wikipedia Last of Us page:
[https://en.wikipedia.org/wiki/The_Last_of_Us](https://en.wikipedia.org/wiki/The_Last_of_Us)

.small-code[
```{r website-select, warning=FALSE}
base_url <- "https://en.wikipedia.org/wiki/The_Last_of_Us"
```
]

--

### Now let's `read_html()` of the webpage

.tiny-code[
```{r read-html, warning=FALSE}
html_base <- base_url %>% 
  read_html() 
```
]

--

.tiny-code[
```{r html-body, eval=FALSE}
html_base %>%
  html_node('body') 
```
]

Now we will use this to scrape the page. 

---

### Class exercise

**Don't look ahead!**

Can you find the CSS selectors of what you want to scrape? For example:
* Title?
* The little Wiki box? 
* Subtitles?
* References?

---

### Let's start with subtitles:

Let's use Inspect or Selector Gadget to find what is the CSS selector for Subtitles:

--

.small-code[
```{r scrape-subtitle, warning=FALSE}
css_subtitle <- "h2" 

subtitles <- html_base %>%
  html_elements(css = css_subtitle) %>%
  html_text2()
subtitles
```
]

--

.tiny-code[
```{r subtitle-class, warning=FALSE}
typeof(subtitles)
```
]

---

### Let's get the text

Use Inspect or Selector Gadget for Text

--

.small-code[
```{r scrape-text, warning=FALSE}
text <- html_base %>%
html_nodes("p") %>% 
  html_text()
head(text)
```
]

---

### Now let's get the links on the page

Once we look at it with inspector, we see that all links are class "a"

.small-code[
```{r scrape-links, warning=FALSE}
links <- html_base %>%
  html_nodes("a") %>%
  html_attr("href")
head(links)
```
]

---

### What else can we do? Can we get the first table?

.small-code[
```{r scrape-table, warnings=FALSE}
css_table <- "#mw-content-text > div.mw-parser-output > table"

table <- html_base %>%
  html_elements(css = css_table) %>%
  html_table() 
table
```
]

---

What if I'm only interested in links of other Wikipedia pages linked? What is the common pattern? * When I checked with CSS selector that is all that starts with /wiki/. 
Let's write a pattern to select only those links

.small-code[
```{r scrape-links2, warning=FALSE}
links2 <- html_base %>%
  html_elements("a") %>%
  html_attr("href") %>%
  str_subset(":", negate = TRUE) %>% 
  str_subset("^#", negate = TRUE) %>% 
  str_subset("^/wiki/") %>% 
  str_subset("/wiki/Main_Page", negate = TRUE) 

head(links2)
```
]

---

Let's also add the "https://en.wikipedia.org/" so it's a complete link, add the title of the page and remove duplicates:
.tiny-code[
```{r add-link, warning=FALSE}
links2 <- html_base %>%
  html_elements("a") %>%
  html_attr("href") %>%
  str_subset(":", negate = TRUE) %>% 
  str_subset("^#", negate = TRUE) %>% 
  str_subset("^/wiki/") %>% 
  str_subset("/wiki/Main_Page", negate = TRUE) %>%
  tibble(.name_repair = ~ c("Title")) %>%
  mutate(links = str_c("https://en.wikipedia.org", Title)) %>%
  distinct() %>%
  mutate(Title = str_replace(Title,".*/wiki/",""))

head(links2)
```
]

---

### Summary: Webscraping using Rvest Steps

1. Inspect the websites hierarchy, and decide what you need
2. Use inspector to select the CSS Selector you will use
3. Read html
4. Use html_nodes, html_elements and html_attributes to select what you need
5. Extract attributes using html_text, html_text2, or html_table

---

## For all questions and comments reach out! 

