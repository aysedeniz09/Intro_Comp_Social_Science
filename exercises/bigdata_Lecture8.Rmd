---
title: "Text as Data: Dictionary Methods and Word Embeddings"
subtitle: "COM EM757"
author: "Dr. Ayse D. Lokmanoglu"
date: 'Lecture 8, (B) March 19, (A) March 24'
output: github_document
always_allow_html: true
editor_options: 
  markdown: 
    wrap: 72
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(
  echo = TRUE,
  message = FALSE, 
  warning = FALSE
)
```

# R Exercises

------------------------------------------------------------------------

**ALWAYS** Let's load our libraries

```{r}
library(tidyverse)
library(tidytext)
library(ggplot2)
library(stopwords)
library(data.table)
library(word2vec)
library(umap)
library(reshape2)
library(wordcloud)
library(text2vec)
library(tm)
library(ggrepel)
library(plotly)
```

## 1. Introduction to Text as Data

Text data:

-   is unstructured,

-   requires preprocessing to be analyzed.

![](https://media.geeksforgeeks.org/wp-content/uploads/20210526142713/BlockDigramofTextMining.png)
*source:
<https://media.geeksforgeeks.org/wp-content/uploads/20210526142713/BlockDigramofTextMining.png>*

| **Phase** | **Technique** | **Core Question** | **Purpose** | **Methods & R Packages** |
|---------------|---------------|---------------|---------------|---------------|
| **Text Preprocessing** | Tokenization | How can we segment text into meaningful units? | Convert text into individual words or phrases. | `tidytext` (`unnest_tokens()`), `stringr` (`str_split()`) |
|  | Stopword Removal | How can we remove redundant words? | Eliminate common words that add little meaning. | `tidytext` (`stop_words`), `tm` (`removeWords()`) |
|  | Lemmatization & Stemming | How can we reduce word variations? | Standardize words to their root forms. | `textstem` (`lemmatize_words()`), `SnowballC` (`wordStem()`) |
| **Feature Engineering** | N-grams | How can we capture word sequences? | Identify multi-word expressions and patterns. | `tidytext` (`unnest_tokens(ngrams = 2)`), `text2vec` |
|  | Part-of-Speech Tagging | How can we recognize word functions? | Assign grammatical categories to words. | `udpipe` (`udpipe_annotate()`), `spacyr` |
| **Content Analysis** | Dictionary-Based Analysis | How can we quantify meaning in text? | Detect linguistic, psychological, or topical patterns. | `tidytext` (`get_sentiments()`), `quanteda` (`dfm_lookup()`) |
| **Machine Learning** | Supervised Classification | How can we predict categories from text? | Assign labels based on prior training examples. | `caret`, `textrecipes`, `tidymodels` |
|  | Unsupervised Clustering | How can we discover hidden patterns? | Group similar documents or topics automatically. | `topicmodels` (LDA), `quanteda` (k-means clustering), `text2vec` (word embeddings) |

We will learn 2 methods today:

1.  **Dictionary Methods** - Using predefined word lists to categorize
    text (e.g., sentiment analysis with lexicons).

2.  **Word Embeddings** - Representing words as numerical vectors to
    capture semantic relationships and similarities.

------------------------------------------------------------------------

## 2. Dictionary Methods

Dictionary-based methods assign predefined categories to words.

### 2.1 Online Dataset: Twitter Data

**Note:** **This dataset is raw and unfiltered, meaning it may contain
explicit language, including swear words. Please proceed with awareness
and discretion.**

We will use a publicly available Twitter Sentiment Analysis Dataset,
which contains tweets labeled as positive, neutral, or negative.

```{r}
twitter_data <- read_csv("https://raw.githubusercontent.com/cardiffnlp/tweeteval/main/datasets/sentiment/test_text.txt", col_names = FALSE) |>
  rename(text = X1) ### renaming the first column as text

head(twitter_data)

```

------------------------------------------------------------------------

### 2.2 Text Preprocessing

Since the dataset consists of a **single column of text**, we need to
ensure proper formatting before tokenization. We will also remove
unnecessary whitespace and convert text to lowercase to maintain
consistency. Also always add an index column to later help with the
merging of the data.

```{r}
# Ensure text is properly formatted
twitter_data <- twitter_data |>
  mutate(textBU = text,   ### created a backup column so we always have the OG text
    text = str_squish(text)) |>
  filter(!is.na(text)) |>
  mutate(text = str_remove_all(text, "@\\S+")) |> # Remove usernames
  mutate(text = str_remove_all(text, "\\d+")) |>  # remove numbers
  mutate(post_index = seq_len(nrow(twitter_data))) |> ### creating an index
  mutate(nwords = str_count(text, "\\w+")) ### counting number of words you will see why

head(twitter_data)
```

Before applying dictionary methods: - Clean the text by: - Removing stop
words and unnecessary characters.

```{r}
# Tokenize text
twitter_tokens <- twitter_data |>
  unnest_tokens(word, text) |>
  anti_join(stop_words, by = "word") ## removing stopwords

# View tokenized words
head(twitter_tokens)
```

------------------------------------------------------------------------

### 2.3 Sentiment Analysis with Dictionary Methods

To understand how different sentiment analysis lexicons classify text,
we will compare results from multiple dictionaries, including **Bing**,
**AFINN**, and **NRC**. Each lexicon provides different insights:

-   **Bing**: Binary classification (positive/negative sentiment).
-   **AFINN**: Numeric scores for sentiment intensity.
-   **NRC**: Categorizes words into emotional dimensions (anger, joy,
    fear, etc.).

**Note: For AFINN you need to select 1 in your console**

```{r}
get_sentiments("afinn")
```

```{r}
get_sentiments("bing")
```

```{r}
get_sentiments("nrc")
```

Let's now see how is it in our dataset

```{r}
# Apply Bing sentiment lexicon
## Step 1:
bing_sentiments_S1 <- twitter_tokens |>
  inner_join(get_sentiments("bing"), by = "word")
head(bing_sentiments_S1)

bing_sentiments_S2 <- bing_sentiments_S1 |> 
  count(post_index, sentiment)
head(bing_sentiments_S2)

bing_sentiments_S3 <- bing_sentiments_S2 |> 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0)

head(bing_sentiments_S3)

bing_sentiments_S4 <- bing_sentiments_S3 |> 
  mutate(sentiment = positive - negative)
head(bing_sentiments_S4)

```

Full Pipe:

```{r}
bing_sentiments <- twitter_tokens |>
  inner_join(get_sentiments("bing"), by = "word") |> 
  count(post_index, sentiment) |> 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |> 
  mutate(sentiment = positive - negative) |> 
  mutate(method = "Bing")

```

Now let's repeat it with AFINN, from now on I am going to give you the
full pipeline, if you want you can see step by step

```{r}

afinn_sentiments <- twitter_tokens |>
  inner_join(get_sentiments("afinn")) |> 
  group_by(post_index) |>  
  summarise(sentiment = sum(value)) |> 
  mutate(method = "AFINN")
```

Now w/ NRC:

```{r}

nrc_sentiments <-  twitter_tokens |> 
    inner_join(get_sentiments("nrc") |> 
                 filter(sentiment %in% c("positive", 
                                         "negative"))) |> 
  count(post_index, sentiment) |> 
  pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |> 
  mutate(sentiment = positive - negative) |> 
  mutate(method = "NRC")

```

------------------------------------------------------------------------

### 2.4 Visualizing and Comparing Sentiment Analysis Results

```{r}
all_sentiments <- bind_rows(afinn_sentiments,
          bing_sentiments,
          nrc_sentiments) |> 
  dplyr::select(-positive, -negative)


ggplot(all_sentiments,
       aes(post_index, sentiment, fill = method)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~method, ncol = 1, scales = "free_y")

```

------------------------------------------------------------------------

### 2.5 Most common positive and negative words

```{r}
bing_word_counts <- twitter_tokens |> 
  inner_join(get_sentiments("bing")) |> 
  count(word, sentiment, sort = TRUE) |> 
  ungroup()

head(bing_word_counts)
```

Visualize it:

```{r}
bing_word_counts |> 
  group_by(sentiment) |> 
  slice_max(n, n = 10) |> 
  ungroup() |> 
  mutate(word = reorder(word, n)) |> 
  ggplot(aes(n, word, fill = sentiment)) +
  geom_col(show.legend = FALSE) +
  facet_wrap(~sentiment, scales = "free_y") +
  labs(x = "Sentiment Count",
       y = NULL)
```

We can also do wordclouds

```{r}
twitter_tokens |> 
  inner_join(get_sentiments("nrc")) |> 
  filter(sentiment == "disgust" | sentiment == "trust") |> ### try to change these try different ones
  count(word, sentiment, sort = TRUE) |> 
  acast(word ~ sentiment, value.var = "n", fill = 0) |> 
  comparison.cloud(colors = c("gray20", "gray80"),
                   max.words = 100)
```

------------------------------------------------------------------------

### 2.6 Normalize sentiment scores

-   What are some ways I can normalize sentiment scores?
    -   Divide by number of words in my document!

```{r}
afinn_sentiments2 <- afinn_sentiments |>
  left_join(twitter_data, by = "post_index") |> 
  group_by(post_index) |>
  mutate(normalized_score = sentiment / nwords)

head(afinn_sentiments2)

ggplot(afinn_sentiments2, aes(x = normalized_score)) +
  geom_histogram() +
  labs(
    title = "Histogram of AFINN Normalized Sentiment Scores",
    x = "AFINN Normalized Scores",
    y = "Count"
  ) +
  theme_minimal()
```

To compare the non-normalized scores:

```{r}
ggplot(afinn_sentiments2, aes(x = sentiment)) +
  geom_histogram() +
  labs(
    title = "Histogram of AFINN  Sentiment Scores",
    x = "AFINN  Scores",
    y = "Count"
  ) +
  theme_minimal()


```

------------------------------------------------------------------------

## 3. Word Embeddings

Word embeddings are a powerful method for representing words as
numerical vectors in a high-dimensional space, capturing semantic
relationships based on context. Unlike dictionary-based sentiment
analysis, word embeddings can recognize nuances in meaning, synonyms,
and relationships between words.

------------------------------------------------------------------------

### 3.1 Introduction to Word Embeddings

-   [**Word2Vec**](https://cran.r-project.org/web/packages/word2vec/readme/README.html):
    Uses a neural network model to generate word embeddings based on
    context.
-   [**GloVe**](https://cran.r-project.org/web/packages/text2vec/vignettes/glove.html):
    Constructs word vectors based on word co-occurrence in a corpus.
-   [**FastText**](https://cran.r-project.org/web/packages/fastText/index.html):
    Extends Word2Vec by representing words as subword n-grams, improving
    performance for rare words.

------------------------------------------------------------------------

#### 3.1.1 Continuous Bag of Words (CBOW)

The **CBOW model** predicts a target word based on the surrounding
context words. It works as follows:

1.  **Input Context**: The model takes a window of words surrounding a
    target word.
2.  **Word Representation**: Each word is mapped to a vector embedding
    that captures its semantic and syntactic properties.
3.  **Aggregation**: The individual word vectors in the context window
    are combined into a single vector.
4.  **Prediction**: The model uses this aggregated vector to predict the
    most probable target word.
5.  **Optimization**: The model is trained to minimize the difference
    between predicted and actual words, refining the vector
    representations over time.

![](https://media.geeksforgeeks.org/wp-content/uploads/20231220164157/Screenshot-2023-12-20-164143.png)
*image from:
<https://media.geeksforgeeks.org/wp-content/uploads/20231220164157/Screenshot-2023-12-20-164143.png>*

For example, given the sentence *"The weather is very nice today"* and a
window size of 2, to predict the word **"nice"**, the context words
**"very"** and **"today"** are used as input.

CBOW is efficient for handling large datasets and is useful for tasks
requiring general word representations.

------------------------------------------------------------------------

#### 3.1.2 Skip-Gram Model

Unlike CBOW, the **Skip-Gram model** works in reverse: it predicts
**context words** given a target word. It works as follows:

1.  **Input Target Word**: The model takes a single word as input.
2.  **Word Representation**: The target word is mapped to a
    high-dimensional vector embedding.
3.  **Probability Distribution**: The model generates probabilities for
    words likely to appear in the surrounding context.
4.  **Context Word Prediction**: Words with the highest probability are
    selected as context words.
5.  **Training Optimization**: The model fine-tunes word embeddings by
    maximizing the probability of correctly predicting surrounding
    words.

![](https://media.geeksforgeeks.org/wp-content/uploads/20231220164505/Screenshot-2023-12-20-164451.png)

*image from:
<https://media.geeksforgeeks.org/wp-content/uploads/20231220164505/Screenshot-2023-12-20-164451.png>*

For example, given the target word **"nice"** in the sentence *"The
weather is very nice today"* (with a window size of 2), the model tries
to predict context words **"very"** and **"today"**.

Skip-Gram performs better on small datasets and captures relationships
between rare words more effectively.

------------------------------------------------------------------------

### 3.2 Applying Word Embeddings in R

We will train a **Word2Vec model** on the Twitter dataset using both
**Continuous Bag of Words (CBOW)** and **Skip-Gram** algorithms to
analyze relationships between words.

*Exercise is from
<https://media.geeksforgeeks.org/wp-content/uploads/20231220164505/Screenshot-2023-12-20-164451.png>*

#### 3.2.1 Training Word2Vec with CBOW

Step 1: Select the text column

```{r}
tweets <- twitter_data$text
```

Step 2: Train a Word2Vec model using the CBOW algorithm

```{r}
cbow_model <- word2vec(x = tweets, type = "cbow", dim = 15, iter = 20)
```

Step 3: Create embeddings using the trained CBOW model and print

```{r}
# checking embeddings
cbow_embedding <- as.matrix(cbow_model)
cbow_embedding <- predict(cbow_model, c("election", "vote"), type = "embedding")
print("The CBOW embedding for election and vote is as follows ")
print(cbow_embedding)
```

Step 4: Find look alikes

```{r}
cbow_lookslike <- predict(cbow_model, c("election", "vote"), 
                          type = "nearest", top_n = 5)
print("The nearest words for election and vote in CBOW model prediction is as follows")
print(cbow_lookslike)
```

#### 3.2.2 VisualizeCBOW

Step 1: Create a corpus of the reviews.

```{r}
text_corpus <- Corpus(VectorSource(tweets))
```

Step 2: Preprocess

```{r}
text_corpus <- Corpus(VectorSource(twitter_data$text))
text_corpus <- tm_map(text_corpus, content_transformer(tolower))
text_corpus <- tm_map(text_corpus, removePunctuation)
text_corpus <- tm_map(text_corpus, removeNumbers)
text_corpus <- tm_map(text_corpus, removeWords, stopwords("en"))
text_corpus <- tm_map(text_corpus, stripWhitespace)
```

Step 3: DTM - and take top 100

```{r}
# Create a document-term matrix
dtm <- DocumentTermMatrix(text_corpus)

# Get the words
words <- colnames(as.matrix(dtm))

# Create a list of words
word_list <- strsplit(words, " ")
word_list <- unlist(word_list)

# Remove empty strings
word_list <- word_list[word_list != ""]

# Take 100
word_list = head(word_list, 100)
```

Step 4: Match the mebeddings

```{r}
# checking embeddings
cbow_embedding <- as.matrix(cbow_model)
cbow_embedding <- predict(cbow_model, word_list, type = "embedding")
cbow_embedding <- na.omit(cbow_embedding)
```

Step 5: Let's do some interactive plots

```{r}
vizualization <- umap(cbow_embedding, n_neighbors = 15, n_threads = 2)

df  <- data.frame(word = rownames(cbow_embedding), 
                  xpos = gsub(".+//", "", rownames(cbow_embedding)), 
                  x = vizualization$layout[, 1], y = vizualization$layout[, 2], 
                  stringsAsFactors = FALSE)

plot_ly(df, x = ~x, y = ~y, type = "scatter", mode = 'text', text = ~word) %>%
  layout(title = "CBOW Embeddings Visualization")
```

------------------------------------------------------------------------

#### 3.2.3 Training Word2Vec with Skip Gram

Step 1: Select the text column:

```{r}
tweets = twitter_data$text
```

Step 2:

```{r}
# using skip gram algorithm
skip_gram_model = word2vec(x = tweets, type = "skip-gram", dim = 15, iter = 20)
```

Step 3: Create embeddings using the `predict()` method provided by
word2vec model. Then print the embeddings of any word.

```{r}
# checking embeddings
skip_embedding <- as.matrix(skip_gram_model)
skip_embedding <- predict(skip_gram_model, c("election", "vote"), type = "embedding")
print("The SKIP Gram embedding for election and vote is as follows ")
print(skip_embedding)
```

Step 4: Find the similar context words that are nearest in meaning by
using the `predict()` method and provide the type as nearest.

```{r}
# finding lookalike
skip_lookslike <- predict(skip_gram_model, c("party", "election"), type = "nearest", 
                          top_n = 5)
print("The nearest words for party and election in skip gram model prediction is as follows ")
print(skip_lookslike)
```

Step 5: Create new embeddings for the words_list. And then draw the
visualization.

```{r}
# checking embeddings
skip_embedding <- as.matrix(skip_gram_model)
skip_embedding <- predict(skip_gram_model, word_list, type = "embedding")
skip_embedding <- na.omit(skip_embedding)


vizualization <- umap(skip_embedding, n_neighbors = 15, n_threads = 2)

df  <- data.frame(word = rownames(skip_embedding), 
                  xpos = gsub(".+//", "", rownames(skip_embedding)), 
                  x = vizualization$layout[, 1], y = vizualization$layout[, 2], 
                  stringsAsFactors = FALSE)

plot_ly(df, x = ~x, y = ~y, type = "scatter", mode = 'text', text = ~word) %>%
  layout(title = "Skig Gram Embeddings Visualization")
```

------------------------------------------------------------------------

## 4. Class Exercises: Sentiment Analysis and Word Embeddings

### Exercise 1: Sentiment Analysis on Twitter Data

-   Load the Starbucks dataset.

```{r}
url <- "https://raw.githubusercontent.com/aysedeniz09/Social_Media_Listening/refs/heads/main/MSC_social_media_list_data/Starbucks_User_Data.csv"
starbucks_user_data <- read_csv(url)
str(starbucks_user_data)
```

-   Apply dictionary-based sentiment analysis using **Bing**, **AFINN**,
    and **NRC**.
-   Compare the results and interpret the findings.
-   Create a **visualization** (bar chart or word cloud) of sentiment
    scores.

### Exercise 2: Exploring Word Embeddings

-   Find a **pre-trained word embedding model** (GloVe, Word2Vec, or
    FastText).
-   Identify **the top 10 most similar words** for "positive" and
    "negative".
-   Visualize word relationships.

### Optional Exercise 3: Combining Sentiment Analysis and Word Embeddings (This is an advanced exercise for those that want to try)

-   Select a subset of the Twitter dataset.
-   Compute sentiment scores using dictionary-based methods.
-   Extract word embeddings for the most frequent words in positive and
    negative tweets.
-   Compare sentiment-based results with word embedding similarities.

------------------------------------------------------------------------

## Lecture 8 Cheat Sheet

### Lecture 8 Cheat Sheet

| **Function/Concept** | **Description** | **Code Example** |
|----------------|----------------------------|----------------------------|
| Tokenization (`unnest_tokens()`) | Breaks text into individual words or phrases for processing. | `twitter_tokens |> unnest_tokens(word, text)` |
| Removing Stopwords (`anti_join(stop_words)`) | Removes common stopwords to focus on meaningful content. | `twitter_tokens |> anti_join(stop_words)` |
| Sentiment Analysis (`get_sentiments()`) | Applies sentiment lexicons (Bing, AFINN, NRC) to categorize words. | `twitter_tokens |> inner_join(get_sentiments('bing'))` |
| Bing Sentiment Analysis (`inner_join(get_sentiments('bing'))`) | Classifies words as positive or negative using the Bing lexicon. | `bing_sentiments |> count(post_index, sentiment)` |
| AFINN Sentiment Analysis (`inner_join(get_sentiments('afinn'))`) | Assigns sentiment scores based on word intensity using AFINN. | `afinn_sentiments |> group_by(text) |> summarize(score = sum(value))` |
| NRC Sentiment Analysis (`inner_join(get_sentiments('nrc'))`) | Categorizes words by emotions such as anger, joy, and fear. | `nrc_sentiments |> count(sentiment)` |
| Word Embeddings - CBOW (`word2vec(type = 'cbow')`) | Trains a Continuous Bag of Words (CBOW) model for word embeddings. | `cbow_model <- word2vec(x = tweets, type = 'cbow', dim = 15, iter = 20)` |
| Word Embeddings - Skip-Gram (`word2vec(type = 'skip-gram')`) | Trains a Skip-Gram model to predict context words from target words. | `skip_gram_model <- word2vec(x = tweets, type = 'skip-gram', dim = 15, iter = 20)` |
| Finding Similar Words (`predict(model, type = 'nearest')`) | Finds words with similar meanings based on trained word embeddings. | `predict(cbow_model, c('election', 'vote'), type = 'nearest')` |
| Extracting Word Embeddings (`predict(model, type = 'embedding')`) | Extracts vector representations of words for further analysis. | `predict(skip_gram_model, c('election', 'vote'), type = 'embedding')` |
| Visualizing Sentiments (`ggplot() + geom_col()`) | Generates bar plots to visualize sentiment distribution in text. | `ggplot(bing_summary, aes(x = sentiment, y = n, fill = sentiment)) + geom_col()` |
| UMAP for Dimensionality Reduction (`umap()`) | Reduces high-dimensional word embeddings for visualization. | `umap_result <- umap(word_embeddings, n_neighbors = 15, n_threads = 2)` |
| Normalize Sentiment Scores (`mutate(normalized_score = score / word_count)`) | Normalizes sentiment scores by dividing by word count. | `afinn_scores |> mutate(normalized_score = score / word_count)` |
| Creating a Sentiment Pipeline (`pivot_wider() + mutate()`) | Combines multiple sentiment analysis steps into a single pipeline. | `bing_sentiments |> pivot_wider(names_from = sentiment, values_from = n, values_fill = 0) |> mutate(sentiment = positive - negative)` |
